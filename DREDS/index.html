<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left; 
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects</h2>
            <h4 style="color:#6e6e6e;"> ECCV 2022 </h4>
            <hr>
            <h6> <a href="" target="_blank">Qiyu Dai</a><sup>1*</sup>, 
                 <a href="https://jiyao06.github.io/" target="_blank">Jiyao Zhang</a><sup>2*</sup>, 
                 <a href="" target="_blank">Qiwei Li</a><sup>1</sup>, 
                 <a href="https://tianhaowuhz.github.io/" target="_blank">Tianhao Wu</a><sup>1</sup>, 
                 <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup>, 
                 <a href="https://scholar.google.de/citations?hl=en&user=pOQmTy0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Ziyuan Liu</a><sup>3</sup>,
                 <a href="https://www.cs.sfu.ca/~pingtan/" target="_blank">Ping Tan</a><sup>3,4</sup>,
                 <a href="https://hughw19.github.io/" target="_blank">He Wang</a><sup>1†</sup>
                 <br>
                 <br>
            <p> <sup>1</sup> Peking University &nbsp;
                <sup>2</sup> Xi’an Jiaotong University &nbsp;
                <sup>3</sup> Alibaba XR Lab &nbsp;
                <sup>4</sup> Simon Fraser University &nbsp;
                <br>
            </p>
            <p> <sup>*</sup> equal contributions &nbsp;
              <sup>†</sup> corresponding author &nbsp;
              <br>
          </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper(Coming soon) </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/PKU-EPIC/DREDS" role="button" target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/PKU-EPIC/DREDS#dataset" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Dataset </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary(Coming soon)</a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="85%"/>
              <br>
            </div>
            <div class="row justify-content-center" style="display:flex;"></div>
              <h6 style="color:#8899a5"> Figure 1. Framework overview (From the left to right): we leverage domain randomization-enhanced depth simulation to generate paired data, on which we can train our depth restoration network SwinDRNet, and the restored depths will be feeded to downstream tasks and improves estimating category-level pose and grasping for specular and transparent objects.</h6>
            </div>
          <p class="text-justify">
            Commercial depth sensors usually generate noisy and missing depths, especially on specular and transparent objects,  which poses critical issues to downstream depth or point cloud-based tasks.
            To mitigate this problem, we propose a powerful RGBD fusion network, SwinDRNet, for depth restoration.
            We further propose Domain Randomization-Enhanced Depth Simulation (DREDS) approach to simulate an active stereo depth system using physics-based rendering and generate a large-scale synthetic dataset that contains 130K photorealistic RGB images along with their simulated depths carrying realistic sensor noises. 
            To evaluate depth restoration methods, we also curate a real-world dataset, namely STD, that captures 30 cluttered scenes composed of 50 objects with different materials from specular, transparent, to diffuse. 
            Experiments demonstrate that the proposed DREDS dataset bridges the sim-to-real domain gap such that, trained on DREDS, our SwinDRNet can seamlessly generalize to other real depth datasets, e.g. ClearGrasp, and outperform the competing methods on depth restoration. We further show that our depth restoration effectively boosts the performance of downstream tasks, including category-level pose estimation and grasping tasks.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Tasks and Results</h3>
            <hr style="margin-top:0px">
            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Depth Restoration</h4>
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/depth_restoration.png" alt="input" class="img-responsive" width="85%"/>
              </div>
              <p class="text-justify">
                Figure 2. We compare our method with several state-of-the-art methods, LIDF and NLSPN. 
                The figure shows the qualitative comparison of STD dataset, 
                demonstrating that our method can predict a more accurate depth on the area with missing or incorrect values while preserving the depth value of the correct area of the raw depth map.
              </p>
            <br>
            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Category-level Pose Estimation</h4>
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/pose_estimation.png" alt="input" class="img-responsive" width="85%"/>
            </div>
            <p class="text-justify">
              Figure 3. The Figure shows the qualitative results of different experiments on DREDS and STD datasets. 
              We can see that the qualities of our predictions are generally better than others. 
              The figure also shows that NOCS [9], SGPA [5] and our method all perform better with the help of restoration depth, 
              especially for specular and transparent objects like the mug, bottle and bowl, which indicates that depth restoration does help category-level pose estimation task.
            </p>
            <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">Robotic Grasping</h4>

        </div>
          
        </div>
      </div>
    </div>
  </section>
  <br>

   <!-- Data -->
   <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>The Proposed Datasets</h3>
          <hr style="margin-top:0px">
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
              <source src="videos/dataset.mp4" type="video/mp4">
            </video>
          </div>
          <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">DREDS (simulated)</h4>
          <p class="text-justify">
            Making use of domain randomization and depth simulation, we construct the large-scale simulated dataset, DREDS.
            In total, DREDS dataset consists of two subsets: 
            1) DREDS-CatKnown: 100,200 training and 19,380 testing RGBD images made of 1,801 objects spanning 7 categories from ShapeNetCore, with randomized specular, transparent, and diffuse materials, 
            2) DREDS-CatNovel: 11,520 images of 60 category-novel objects, which is transformed from GraspNet-1Billion that contains CAD models and annotates poses, by changing their object materials to specular or transparent, to verify the ability of our method to generalize to new object categories.
          </p>
          <h4 style="margin-top:20px; margin-bottom:20px; color:#717980">STD (real)</h4>
          <p class="text-justify">
            To further examine the proposed method in real scenes, we curate a real-world dataset, STD, composing of Specular, Transparent, and Diffuse objects.
            Similar to DREDS dataset, STD dataset contains: 
            1) STD-CatKnown: 27000 RGBD images of 42 category-level objects spanning 7 categories, captured from 25 different scenes with various backgrounds and illumination.
            2) STD-CatNovel: 11000 data of 8 category-novel objects from 5 scenes, for evaluating the generalization ability of the proposed method.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- DREDS -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Domain Randomization-Enhanced Depth Simulation</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
              <source src="videos/dreds.mp4" type="video/mp4">
            </video>
        </div>
          <p class="text-justify">
            we propose to synthesize depths with realistic sensor noise patterns by simulating an active stereo depth camera. Our simulator is built on Blender and leverages raytracing to mimic the IR stereo patterns and compute the depths from them. 
            To facilitate generalization, we further adopt domain randomization techniques that randomize the object textures, object materials (from specular, transparent, to diffuse), object layout, floor textures, illuminations along camera poses.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- Method -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>SwinDRNet for Depth Restoration</h3>
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/SwinDRNet.png" alt="input" class="img-responsive" width="85%"/>
              </div>
            <br>
        </div>
          <p class="text-justify">
            Figure 4. To restore the noisy and incomplete depth, we propose a SwinTransformer based depth restoration network, namely SwinDRNet.
            SwinDRNet takes as input a RGB image Ic along with its aligned depth image Id and outputs a refined depth ˆId that restores the error area of the depth image and completes the invalid area.
            We, for the first time, devise a homogeneous and mirrored architecture that only leverages SwinT to extract and hierarchically fuse the RGB and depth features.
            We first extract the multi-scale features of RGB and depth image in the phase 1, respectively. 
            Next, in the phase 2, our network fuse features of different modalities.
            Finally, generate the initial depth map and confidence maps by two decoders, and fuse the raw depth and initial depth using the predicted confidence map.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{dai2022dreds,
	title={Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects},
	author={Dai, Qiyu and Zhang, Jiyao and Li, Qiwei and Wu, Tianhao and Dong, Hao and Liu, Ziyuan and Tan, Ping and Wang, He},
	booktitle={European Conference on Computer Vision (ECCV)},
	year={2022}
}</code></pre>
      </div>
    </div>
  </div>


    <!-- Contact -->
    <div class="container">
      <div class="row ">
        <div class="col-12">
            <h3>Contact</h3>
            <hr style="margin-top:0px">
            <p>If you have any questions, please feel free to contact Qiyu Dai at qiyudai_at_pku_edu_cn, Jiyao Zhang at zhangjiyao_at_stu_xjtu_edu_cn, and He Wang at hewang_at_pku_edu_cn </p>
        </pre>
        </div>
      </div>
    </div>



  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
