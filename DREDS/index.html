<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <meta name="description" content="NeuralRecon-W reconstructs 3D scene geometry from a monocular video with known camera poses in real-time. Accepted in CVPR 2021 as oral."/> -->
    <title>Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left; 
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects</h2>
            <h4 style="color:#6e6e6e;"> ECCV 2022 </h4>
            <hr>
            <h6> <a href="https://daiqy.github.io" target="_blank">Qiyu Dai</a><sup>1*</sup>, 
                 <a href="https://jiyao06.github.io/" target="_blank">Jiyao Zhang</a><sup>2*</sup>, 
                 <a href="" target="_blank">Qiwei Li</a><sup>1</sup>, 
                 <a href="https://tianhaowuhz.github.io/" target="_blank">Tianhao Wu</a><sup>1</sup>, 
                 <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup>, 
                 <a href="https://scholar.google.de/citations?hl=en&user=pOQmTy0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Ziyuan Liu</a><sup>3</sup>,
                 <a href="https://www.cs.sfu.ca/~pingtan/" target="_blank">Ping Tan</a><sup>4</sup> 
                 <a href="https://hughw19.github.io/" target="_blank">He Wang</a><sup>1†</sup>
                 <br>
                 <br>
            <p> <sup>1</sup> Peking University &nbsp;
                <!-- <a href="https://idr.ai" style="color:#212529">Image Derivative Inc.</a>&nbsp;  -->
                <sup>2</sup> Xi’an Jiaotong University &nbsp;
                <sup>3</sup> Alibaba AI Labs &nbsp;
                <sup>4</sup> Simon Fraser University &nbsp;
                <br>
            </p>
            <p> <sup>*</sup> equal contributions &nbsp;
              <sup>†</sup> corresponding author &nbsp;
              <br>
          </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/PKU-EPIC/DREDS" role="button" target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/PKU-EPIC/DREDS#dataset" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Dataset </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary</a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <!-- <div class="col-md-4">  -->
               <img src="images/teaser.png" alt="input" class="img-responsive" width="70%"/>
                <!-- </div> -->
                <!-- <div class="col-md-8"> 
                  <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid">
                        <source src="videos/neuconw-teaser-bg.m4v" type="video/mp4">
                  </video>
                </div> -->
              </div>
            <!-- <div class="col-md-6 img-responsive">
              <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid">
                    <source src="videos/neuconw-teaser-bg.m4v" type="video/mp4">
              </video>
            </div> -->
            <br>
        </div>
          <p class="text-justify">
            Commercial depth sensors usually generate noisy and missing depths, especially on specular and transparent objects,  which poses critical issues to downstream depth or point cloud-based tasks.
            To mitigate this problem, we propose a powerful RGBD fusion network, SwinDRNet, for depth restoration.
            We further propose Domain Randomization-Enhanced Depth Simulation (DREDS) approach to simulate an active stereo depth system using physics-based rendering and generate a large-scale synthetic dataset that contains 130K photorealistic RGB images along with their simulated depths carrying realistic sensor noises. 
            To evaluate depth restoration methods, we also curate a real-world dataset, namely STD, that captures 30 cluttered scenes composed of 50 objects with different materials from specular, transparent, to diffuse. 
            Experiments demonstrate that the proposed DREDS dataset bridges the sim-to-real domain gap such that, trained on DREDS, our SwinDRNet can seamlessly generalize to other real depth datasets, e.g. ClearGrasp, and outperform the competing methods on depth restoration. We further show that our depth restoration effectively boosts the performance of downstream tasks, including category-level pose estimation and grasping tasks.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{dai2022dreds,
	title={Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects},
	author={Dai, Qiyu and Zhang, Jiyao and Li, Qiwei and Wu, Tianhao and Dong, Hao and Liu, Ziyuan and Tan, Ping and Wang, He},
	booktitle={European Conference on Computer Vision (ECCV)},
	year={2022}
}</code></pre>
      </div>
    </div>
  </div>



  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
