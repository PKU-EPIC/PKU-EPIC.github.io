<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Tracking and Reconstructing Hand Object
      Interactions from Point Cloud Sequences in the Wild</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left; 
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Tracking and Reconstructing Hand Object
              Interactions from Point Cloud Sequences in the Wild</h2>
            <h4 style="color:#6e6e6e;"> AAAI 2023 (Oral Presentation)</h4>
            <hr>
            <h6> <a href="https://jychen18.github.io/">Jiayi Chen</a><sup>1, 2*</sup>&nbsp; &nbsp; 
              <a href="https://miyandoris.github.io/"> Mi Yan </a><sup>1*</sup>&nbsp; &nbsp; 
              <a href="https://dblp.org/pid/243/2982.html/">Jiazhao Zhang</a><sup>3, 4</sup>&nbsp; &nbsp; 
              <a href="https://xyz-99.github.io/">Yinzhen Xu</a><sup>1,2</sup>&nbsp; &nbsp;
              <a href="https://dragonlong.github.io/">Xiaolong Li</a><sup>3</sup>&nbsp; &nbsp; 
              <a href="https://yijiaweng.github.io/">Yijia Weng</a><sup>4</sup>&nbsp; &nbsp; 
              <a href="https://ericyi.github.io/">Li Yi</a><sup>5</sup>&nbsp; &nbsp; 
              <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a><sup>6</sup>&nbsp; &nbsp; 
              <a href="https://hughw19.github.io/">He Wang</a><sup>1†</sup>
              <br>
              <br>
            <p> <sup>1</sup>CFCS, Peking University&nbsp; &nbsp; 
              <sup>2</sup>Beijing Institute for General AI&nbsp; &nbsp; 
              <sup>3</sup>Virginia Tech&nbsp; &nbsp;  
              <sup>4</sup>Stanford University&nbsp; &nbsp; 
              <sup>5</sup>Tsinghua University&nbsp; &nbsp;
              <sup>6</sup>Columbia University
                <br>
            </p>
            <p> <sup>*</sup> equal contributions &nbsp;
              <sup>†</sup> corresponding author &nbsp;
              <br>
          </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="http://arxiv.org/abs/2209.12009" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/PKU-EPIC/HOTrack" role="button" target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://mirrors.pku.edu.cn/dl-release/HOTrack_AAAI2023/" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Dataset </a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12 text-center">
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="95%"/>
              <br>
            </div>
            <p class="text-justify">
              <strong>Figure 1. </strong>Left: We generate a large-scale hand-object interaction dataset, named SimGrasp, using
              simulated structure light based depth sensor. Right: Trained only on SimGrasp, our methods can be
              directly transferred to the challenging real world datasets, i.e. HO3D and DexYCB, to track
              and reconstruct hand object interaction.
            </p>
              <!-- </div> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- abstract -->
  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Abstract</strong></h2>
            <hr style="margin-top:0px">
              <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="95%"/>
              <br>
            </div> -->
          <!-- <p class="text-justify">
              <h6 style="color:#8899a5;text-align:left"> Figure 1. Framework overview (From the left to right): we leverage domain randomization-enhanced depth simulation to generate paired data, on which we can train our depth restoration network SwinDRNet, and the restored depths will be fed to downstream tasks and improves estimating category-level pose and grasping for specular and transparent objects.</h6>
          </p> -->
            <p class="text-justify">
              In this work, we tackle the challenging task of jointly tracking hand object pose
              and reconstructing their shapes from depth point cloud sequences in the wild, given
              the initial poses at frame 0. We for the first time propose a point cloud based
              hand joint tracking network, HandTrackNet, to estimate the inter-frame hand joint
              motion. Our HandTrackNet proposes a novel hand pose canonicalization module
              to ease the tracking task, yielding accurate and robust hand joint tracking. Our
              pipeline then reconstructs the full hand via converting the predicted hand joints into
              a template-based parametric hand model MANO. For object tracking, we devise a
              simple yet effective module that estimates the object SDF from the first frame and
              performs optimization-based tracking. Finally, a joint optimization step is adopted
              to perform joint hand and object reasoning, which alleviates the occlusion-induced
              ambiguity and further refines the hand pose. During training, the whole pipeline
              only sees purely synthetic data, which are synthesized with sufficient variations and
              by depth simulation for the ease of generalization. The whole pipeline is pertinent
              to the generalization gaps and thus directly transferable to real in-the-wild data. We
              evaluate our method on two real hand object interaction datasets, e.g. HO3D and
              DexYCB, without any finetuning. Our experiments demonstrate that the proposed
              method significantly outperforms the previous state-of-the-art depth-based hand
              and object pose estimation and tracking methods, running at a frame rate of 9 FPS.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Methods</strong></h2>
            <hr style="margin-top:0px">
            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Full pipeline</b></h3>
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/method.png" alt="input" class="img-responsive" width="95%"/>
              </div>
              <p class="text-justify">
                <strong>Figure 2. At frame 0, We initialize the object shape <sub>O</sub>M<sup>obj</sup> represented in signed
                  distance function (SDF) and the hand shape code β for the parametric model MANO, as shown in the
                  dotted line. In the following tracking phase, at each frame t, we first separately estimate the object
                  pose {R<sub>t</sub><sup style="margin-left:-5px">obj</sup>, T<sub>t</sub><sup style="margin-left:-5px">obj</sup>} and hand pose {R<sub>t</sub><sup style="margin-left:-5px">hand</sup>, T<sub>t</sub><sup style="margin-left:-5px">hand</sup>, θ<sub>t</sub>}, and further refine the hand pose by taking
                    hand-object interaction into consideration. We can also update object shape and hand shape every 10 frames, as shown in our supplementary materials.
              </p>
            <br>
            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>HandTrackNet</b></h3>
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/network.png" alt="input" class="img-responsive" width="85%"/>
            </div>
            <p class="text-justify">
              <b>Figure 3. HandTrackNet takes input the hand points Pt from
                the current frame t and the estimated hand joints J<sub>t-1</sub> from the last frame, and perform global
                pose canonicalization to both of the two. Then it leverage PointNet++ to extract features from
                canonicalized hand points P<sub>t</sub><sup style="margin-left:-5px">'</sup>
                and use each joint J<sub>t</sub><sup style="margin-left:-5px">coarse'</sup>
                to query and pass features, followed by a
                MLP to regress and update joint positions.
            </p>
            <br>
            <!-- <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Robotic Grasping</b></h3> -->
            <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
              <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
                <source src="videos/grasping.mp4" type="video/mp4">
              </video>
            </div>   -->
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  
  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>SimGrasp Dataset</strong></h2>
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/simgrasp.png" alt="input" class="img-responsive" width="95%"/>
              </div>
              <p class="text-justify">
                <strong>Figure 4. We synthesize a large-scale dynamic dataset SimGrasp with
                  sufficient variations and realistic sensor noises.
              </p>
            <br>
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Video</strong></h2>
            <hr style="margin-top:0px">
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <video width="80%" playsinline="" preload="" muted="" controls>
                <source src="videos/camera_ready.mp4" type="video/mp4">
              </video>
            </div>  
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Qualitative results</strong></h2>
          <hr style="margin-top:0px">
          <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>HO3D</b></h3>
          <img src="videos/GPMF13_0000.gif" width="80%" style="margin-bottom:20px;">
          <img src="videos/sm3-0000.gif" width="80%" style="margin-bottom:20px;">
          <img src="videos/mc4-0000.gif" width="80%" style="margin-bottom:20px;">
          <p class="text-justify">
            From left to right: input point cloud sequences, output overlay with RGB, output, output from another view. 
            The speed of the video is consistent with the real time.
          </p>
          <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>DexYCB</b></h3>
          <img src="videos/20200903-113012-932122060861-000023.gif" width="80%" style="margin-bottom:20px;">
          <img src="videos/20200928-154232-932122061900-000025.gif" width="80%" style="margin-bottom:20px;">
          <img src="videos/20201022-110947-932122061900-000023.gif" width="80%" style="margin-bottom:20px;">
          <p class="text-justify">
          From left to right: input point cloud sequences, output overlay with RGB, output, output from another view.
          The speed of the video is consistent with the real time.
          </p>
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <!-- citing -->
  <div class="container" style="width:58%">
    <div class="row ">
      <div class="col-12">
          <h2><strong>Citation</strong></h2>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{chen2022tracking,
  title={Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild},
  author={Chen, Jiayi and Yan, Mi and Zhang, Jiazhao and Xu, Yinzhen and Li, Xiaolong and Weng, Yijia and Yi, Li and Song, Shuran and Wang, He},
  journal={arXiv preprint arXiv:2209.12009},
  year={2022}
}</code></pre>
      </div>
    </div>
  </div>
  <br>

    <!-- Contact -->
    <div class="container" style="width:58%">
      <div class="row ">
        <div class="col-12">
            <h2><strong>Contact</strong></h2>
            <hr style="margin-top:0px">
            <p>If you have any questions, please feel free to contact <b>Jiayi Chen</b> at jiayichen_at_pku_edu_cn, <b>Mi Yan</b> at dorisyan_at_pku_edu_cn, and <b>He Wang</b> at hewang_at_pku_edu_cn </p>
        </pre>
        </div>
      </div>
    </div>



  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
