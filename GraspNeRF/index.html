<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GraspNeRF</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left; 
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF</h2>
            <h4 style="color:#6e6e6e;"> ICRA 2023 </h4>
            <hr>
            <h6> <a href="https://daiqy.github.io/" target="_blank">Qiyu Dai</a><sup>1,2*</sup>, 
                 <a href="https://github.com/fzy139" target="_blank">Yan Zhu</a><sup>1*</sup>, 
                 <a href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a><sup>1</sup>, 
                 <a href="https://github.com/softword-tt/" target="_blank">Ciyu Ruan</a><sup>3</sup>, 
                 <a href="https://jzhzhang.github.io/" target="_blank">Jiazhao Zhang</a><sup>1,2</sup>, 
                 <a href="https://hughw19.github.io/" target="_blank">He Wang</a><sup>1†</sup>
                 <br>
                 <br>
            <p> <sup>1</sup> Peking University &nbsp;
                <sup>2</sup> Beijing Academy of Artificial Intelligence &nbsp;
                <sup>3</sup> National University of Defense Technology &nbsp;

                <br>
            </p>
            <p> <sup>*</sup> equal contributions &nbsp;
              <sup>†</sup> corresponding author &nbsp;
              <br>
          </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2210.06575" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://github.com/PKU-EPIC/GraspNeRF" role="button" target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code(Coming soon) </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Data Generator(Coming soon) </a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary(Coming soon)</a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12 text-center">
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="50%"/>
              <br>
            </div>
            <!-- <div class="row justify-content-center" style="display:flex;"></div> -->
            <p class="text-justify">
			  <strong>Figure 1. Overview of the proposed GraspNeRF and the dataset.</strong> Our method takes sparse multiview RGB images as input, constructs a neural radiance field, and executes material-agnostic grasp detection within 90ms. We train the model on the proposed large-scale synthetic multiview grasping dataset generated by photorealistic rendering and domain randomization.
            </p>
              <!-- </div> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- abstract -->
  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Abstract</strong></h2>
            <hr style="margin-top:0px">
              <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="95%"/>
              <br>
            </div> -->
          <!-- <p class="text-justify">
              <h6 style="color:#8899a5;text-align:left"> Figure 1. Framework overview (From the left to right): we leverage domain randomization-enhanced depth simulation to generate paired data, on which we can train our depth restoration network SwinDRNet, and the restored depths will be fed to downstream tasks and improves estimating category-level pose and grasping for specular and transparent objects.</h6>
          </p> -->
            <p class="text-justify">
            In this work, we tackle 6-DoF grasp detection for transparent and specular objects, which is an important yet challenging problem in vision-based robotic systems, due to the failure of depth cameras in sensing their geometry. 
			We, for the first time, propose a multiview RGB-based 6-DoF grasp detection network, GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to achieve material-agnostic object grasping in clutter. 
			Compared to the existing NeRF-based 3-DoF grasp detection methods that rely on densely captured input images and time-consuming per-scene optimization, our system can perform zero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF grasps, both in real-time. 
			The proposed framework jointly learns generalizable NeRF and grasp detection in an end-to-end manner, optimizing the scene representation construction for the grasping.
			For training data, we generate a large-scale photorealistic domain-randomized synthetic dataset of grasping in cluttered tabletop scenes that enables direct transfer to the real world.
			Our extensive experiments in synthetic and real-world environments demonstrate that our method significantly outperforms all the baselines in all the experiments while remaining in real-time.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>
  
	<section>
	  <div class="container" style="width:58%">
		<div class="row">
		  <div class="col-12">
			<h2><strong>Video</strong></h2>
			<hr style="margin-top:0px">
			<div class="row justify-content-center" style="align-items:center; display:flex;">
			  <video width="90%" playsinline="" preload="" muted="" controls>
				<source src="videos/graspnerf_supp_cr_lowsize_v2.mp4" type="video/mp4">
			  </video>
			</div>
		  </div>
		</div>
	  </div>
	  </div>
	</section>
	<br>
	<br>

  <!-- Results -->
  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Tasks and Results</strong></h2>
            <hr style="margin-top:0px">
            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Simulation Grasping Experiments</b></h3>
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/sim_single.png" alt="input" class="img-responsive" width="65%"/>
              </div>
              <p class="text-justify">
                <strong>Figure 2. Success rates of single object retrieval in simulation.</strong> Our method achieves an average success rate of 86.1% over 36 trials, outperforming all the baselines.
              </p>
				<br>
			  <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/sim_multi.png" alt="input" class="img-responsive" width="65%"/>
              </div>
              <p class="text-justify">
                <strong>Figure 3. Results of sequential clutter removal in simulation. Each cell: transparent and specular (mixed).</strong>
				Our method exhibits superior performance for sequential clutter removal, 
				where we evaluate the methods on various objects with transparent and specular as well as mixed materials. 
				Specifically, our method significantly improves the performance of transparent and specular object grasping compared to other methods.
              </p>
				<br>
			
			
            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Real Robot Experiments</b></h3>
            <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/real_single.png" alt="input" class="img-responsive" width="65%"/>
              </div>
              <p class="text-justify">
                <strong>Figure 4. Success rates of real-world single object retrieval.</strong> GraspNeRF obtains 88.9% of 18 trials and beats competing baselines.
              </p>
				<br>
			  <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/real_multi.png" alt="input" class="img-responsive" width="65%"/>
              </div>
              <p class="text-justify">
                <strong>Figure 5. Results of real-world sequential clutter removal. Each cell: transparent and specular (mixed).</strong>
				GraspNeRF achieves the highest grasping success rate and declutter rate in various materials in both pile and packed settings.
				Compared to NeRF-VGN (49 views), GraspNeRF only requires 6 views. Moreover, our method runs inference at 11 FPS without any optimization, 
				which is significantly faster than NeRF-VGN with per-scene optimization. 
				Importantly, our method successfully generalizes to new categories, as evidenced by the results for the two transparent objects (Glass gourd and small wine glass). 
			  </p>
				<br>
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>
  
  <!-- Method -->
    <section>
      <div class="container" style="width:58%">
        <div class="row">
          <div class="col-12">
            <h2><strong>Methods</strong></h2>
              <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/pipeline.png" alt="input" class="img-responsive" width="98%"/>
              </div>
              <br>
              <p class="text-justify">
                <b>Figure 6. Overview of GraspNeRF.</b>               
				Our proposed framework is composed of a scene representation construction module and a volumetric grasp detection module.
              In scene representation construction module, we first extract and aggregate the multiview image features for two proposes: 
              the extracted geometric features form a feature grid and will be passed to our TSDF prediction network 
              to construct the TSDF of the scene, which encodes the scene geometry as well as the density of the underlying
               NeRF; at the same time, the features are used to predict color, which along with the density outputs enables the NeRF rendering.
              Taking the predicted TSDF as input, our volumetric grasp detection module then learns to predict 6-DoF grasps 
              at each voxel of the TSDF.
              </p>
          </div>
          </div>
        </div>
      </div>
    </section>
    <br>

   <!-- Data -->
   <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Multiview 6-DoF Grasping Dataset</strong></h2>
		  <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/dataset.png" alt="input" class="img-responsive" width="98%"/>
          </div>
          <p class="text-justify">
		  <b>Figure 7. Examples of our dataset.</b>
		  Our data generation pipeline is motivated by DREDS. To construct the scenes, 
          we transform an existing grasping dataset from GIGA that contains CAD models 
          and annotates object poses, by changing their object materials to diffuse, specular or transparent. 
          Then we render photorealistic multiview RGBs on Blender.
			To bridge the sim2real gap, we leverage domain randomization, which randomizes object materials and textures, 
			backgrounds, illuminations, and camera poses. After training on the synthetic dataset with sufficient variations, 
		the network considers real data as a variation of training data in testing time, so as to generalize to real.
          </p>
          <br>
        </div>
      </div>
    </div>
  </section>
  <br>

    

  <!-- Team -->
  <!--section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Team</strong></h2>
          <hr style="margin-top:0px">
          <table style="width:100%">
            <p>
            <tr>
              <td> <center> <a href="https://daiqy.github.io/" target="_blank"> <img alt src="images/team_qiyudai.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://github.com/Jiyao06" target="_blank"> <img alt src="images/team_jiyaozhang.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://github.com/qiweili00" target="_blank"> <img alt src="images/team_qiweili.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://tianhaowuhz.github.io/" target="_blank"> <img alt src="images/team_tianhaowu.png" height="100"/> </a></center> </td>
              <td> <center> <a href="https://zsdonghao.github.io/" target="_blank"> <img alt src="images/team_haodong.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://scholar.google.de/citations?hl=en&user=pOQmTy0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank"> <img alt src="images/team_ziyuanliu.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://www.cs.sfu.ca/~pingtan/" target="_blank"> <img alt src="images/team_pingtan.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://hughw19.github.io/" target="_blank"> <img alt src="images/team_hewang.png" height="100"/> </a> </center></td>
            </tr>
            <tr>
              <td>  <center>  <font size= "2">Qiyu Dai<sup>1*</sup></font> </center></td>
              <td>  <center>  <font size= "2">Jiyao Zhang<sup>2*</sup></font></center> </td>
              <td>  <center>  <font size= "2">Qiwei Li<sup>1</sup></font></center> </td>
              <td>  <center>  <font size= "2">Tianhao Wu<sup>1</sup></font> </center></td>
              <td>  <center>  <font size= "2">Hao Dong<sup>1</sup></font> </center></td>
              <td>  <center>  <font size= "2">Ziyuan Liu<sup>3</sup></font> </center></td>
              <td>  <center>  <font size= "2">Ping Tan<sup>3,4</sup></font> </center></td>
              <td> <center>   <font size= "2">He Wang<sup>1†</sup></font> </center></td>
            </tr>
            </p>
            </table>
        </div>
        <div class="col-12">
          <p> <font size= "2"><sup>1</sup> Peking University</font> &nbsp;
            <font size= "2"><sup>2</sup> Xi’an Jiaotong University</font> &nbsp;
            <font size= "2"><sup>3</sup> Alibaba XR Lab</font> &nbsp;
            <font size= "2"><sup>4</sup> Simon Fraser University</font> &nbsp;
            <br>
            <font size= "2"><sup>*</sup> equal contributions</font> &nbsp;
            <font size= "2"><sup>†</sup> corresponding author</font> &nbsp;
          </p>
        </div>
      </div>
    </div>
  </section-->
  <br>
            
  <!-- citing -->
  <div class="container" style="width:58%">
    <div class="row ">
      <div class="col-12">
          <h2><strong>Citation</strong></h2>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{Dai2023GraspNeRF,
  title={GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF},
  author={Qiyu Dai and Yan Zhu and Yiran Geng and Ciyu Ruan and Jiazhao Zhang and He Wang},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2023}
}</code></pre>
      </div>
    </div>
  </div>
  <br>

    <!-- Contact -->
    <div class="container" style="width:58%">
      <div class="row ">
        <div class="col-12">
            <h2><strong>Contact</strong></h2>
            <hr style="margin-top:0px">
            <p>If you have any questions, please feel free to contact 
              <b>Qiyu Dai</b> at qiyudai at pku dot edu dot cn, 
              <b>Yan Zhu</b> at zhuyan_ at stu dot pku dot edu dot cn,
              and <b>He Wang</b> at hewang at pku dot edu dot cn </p>
        </pre>
        </div>
      </div>
    </div>



  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
