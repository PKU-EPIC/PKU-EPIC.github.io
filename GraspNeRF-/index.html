<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GraspNeRF</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left; 
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF</h2>
            <h4 style="color:#6e6e6e;"> ICRA 2023 </h4>
            <hr>
            <h6> <a href="https://daiqy.github.io/" target="_blank">Qiyu Dai</a><sup>1,2*</sup>, 
                 <a href="https://github.com/fzy139" target="_blank">Yan Zhu</a><sup>1*</sup>, 
                 <a href="https://gengyiran.github.io/" target="_blank">Yiran Geng</a><sup>1</sup>, 
                 <a href="https://github.com/softword-tt/" target="_blank">Ciyu Ruan</a><sup>3</sup>, 
                 <a href="https://jzhzhang.github.io/" target="_blank">Jiazhao Zhang</a><sup>1,2</sup>, 
                 <a href="https://hughw19.github.io/" target="_blank">He Wang</a><sup>1,2†</sup>
                 <br>
                 <br>
            <p> <sup>1</sup> Peking University &nbsp;
                <sup>2</sup> Beijing Academy of Artificial Intelligence &nbsp;
                <sup>3</sup> National University of Defense Technology &nbsp;

                <br>
            </p>
            <p> <sup>*</sup> equal contributions &nbsp;
              <sup>†</sup> corresponding author &nbsp;
              <br>
          </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2210.06575" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="" role="button" target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code(Coming soon) </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Dataset(Coming soon) </a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary(Coming soon)</a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12 text-center">
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="95%"/>
              <br>
            </div>
            <!-- <div class="row justify-content-center" style="display:flex;"></div> -->
            <p class="text-justify">
              <strong>Overview of the proposed GraspNeRF and the dataset. Our method takes sparse multiview RGB images as input, constructs a neural radiance field, and executes material-agnostic grasp detection within 90ms. We train the model on the proposed large-scale synthetic multiview grasping dataset generated by photorealistic rendering and domain randomization.
            </p>
              <!-- </div> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- abstract -->
  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Abstract</strong></h2>
            <hr style="margin-top:0px">
            In this work, we tackle 6-DoF grasp detection for transparent and specular objects, which is an important yet challenging problem in vision-based robotic systems, due to the failure of depth cameras in sensing their geometry. 
We, for the first time, propose a multiview RGB-based 6-DoF grasp detection network, GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to achieve material-agnostic object grasping in clutter. 
Compared to the existing NeRF-based 3-DoF grasp detection methods that rely on densely captured input images and time-consuming per-scene optimization, our system can perform zero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF grasps, both in real-time. 
The proposed framework jointly learns generalizable NeRF and grasp detection in an end-to-end manner, optimizing the scene representation construction for the grasping.
For training data, we generate a large-scale photorealistic domain-randomized synthetic dataset of grasping in cluttered tabletop scenes that enables direct transfer to the real world.
Our extensive experiments in synthetic and real-world environments demonstrate that our method significantly outperforms all the baselines in all the experiments while remaining in real-time.
              <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="95%"/>
              <br>
            </div> -->
          <!-- <p class="text-justify">
              <h6 style="color:#8899a5;text-align:left"> Figure 1. Framework overview (From the left to right): we leverage domain randomization-enhanced depth simulation to generate paired data, on which we can train our depth restoration network SwinDRNet, and the restored depths will be fed to downstream tasks and improves estimating category-level pose and grasping for specular and transparent objects.</h6>
          </p> -->

        </div>
        <p class="text-justify">
          <div class="row justify-content-center" style="align-items:center; display:flex;">
          <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
            <source src="videos/graspnerf_submit.mp4" type="video/mp4">
          </video></div>
      </div>

    </div>



  </section>
  <br>

  <!-- Results -->
  <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Material-agnostic grasp detection</strong></h2>
            <hr style="margin-top:0px">
            Despite the great progress in diffuse objects, grasping transparent and specular objects,
             which are ubiquitous in our daily life and need to be handled by robotic systems, is still very challenging.
          Depth sensors struggle to sense those transparent and specular objects and usually generate wrong or even missing depths, 
          thus further leading to the failure of grasping in those methods.

          DexNeRF provides a valuable alternative that only takes RGB inputs,
reconstructing the scene with a NeRF, the powerful implicit 3D representation that encodes scene geometry using an MLP. However
Its NeRF construction needs time-consuming per-scene training that takes at least  hours, which deviates far from the desired real-time running.

<p class="text-justify">

<h2><strong>Generalizable NeRF</strong></h2>
<hr style="margin-top:0px">
To mitigate these issues, we propose to leverage generalizable NeRF, which train on many different scenes and learn to aggregate multiview observations and zero-shot construct NeRFs for novel scenes without training. 
Moreover, it only requires sparse input views, which is more practical for real-world applications.
The crucial demand of grasping is to wholly and accurately represent the scene, especially for transparent and specular objects, in a generalizable and grasping-oriented way.
A vanilla NeRF needs scene-specific training and learns an MLP to overfit one scene without having an explicit representation.
We instead propose to leverage generalizable NeRF that learns to aggregate observations from multiple views and forms a volumetric feature grid. 
         
            </div>  
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>


    <!-- Method -->
    <section>
      <div class="container" style="width:58%">
        <div class="row">
          <div class="col-12">
            <h2><strong>Methods Overview</strong></h2>
            <hr style="margin-top:0px">
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img src="images/pipeline.png" alt="input" class="img-responsive" width="98%"/>
            </div>
              <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
              </div>
              Our proposed framework is composed of a scene representation construction module 
             and a volumetric grasp detection module.
              In scene representation construction module, we first extract and aggregate the multiview image features for two proposes: 
              the extracted geometric features form a feature grid and will be passed to our TSDF prediction network 
              to construct the TSDF of the scene, which encodes the scene geometry as well as the density of the underlying
               NeRF; at the same time, the features are used to predict color, which along with the density outputs enables the NeRF rendering.
              Taking the predicted TSDF as input, our volumetric grasp detection module then learns to predict 6-DoF grasps 
              at each voxel of the TSDF.
              <br>

              <br>
             
          </div>
          </div>
        </div>
      </div>
    </section>
    <br>
     <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>The Proposed Datasets</strong></h2>
          <hr style="margin-top:0px">
          Our data generation pipeline is motivated by DREDS. To construct the scenes, 
          we transform an existing grasping dataset from GIGA that contains CAD models 
          and annotates object poses, by changing their object materials to diffuse, specular or transparent. 
          Then we render photorealistic multiview RGBs on Blender.
To bridge the sim2real gap, we leverage domain randomization, which randomizes object materials and textures, 
backgrounds, illuminations, and camera poses. After training on the synthetic dataset with sufficient variations, 
the network considers real data as a variation of training data in testing time, so as to generalize to real.
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- Team -->
  <!-- <section>
    <div class="container" style="width:58%">
      <div class="row">
        <div class="col-12">
          <h2><strong>Team</strong></h2>
          <hr style="margin-top:0px">
          <table style="width:100%">
            <p>
            <tr>
              <td> <center> <a href="https://daiqy.github.io/" target="_blank"> <img alt src="images/team_qiyudai.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://github.com/Jiyao06" target="_blank"> <img alt src="images/team_jiyaozhang.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://github.com/qiweili00" target="_blank"> <img alt src="images/team_qiweili.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://tianhaowuhz.github.io/" target="_blank"> <img alt src="images/team_tianhaowu.png" height="100"/> </a></center> </td>
              <td> <center> <a href="https://zsdonghao.github.io/" target="_blank"> <img alt src="images/team_haodong.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://scholar.google.de/citations?hl=en&user=pOQmTy0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank"> <img alt src="images/team_ziyuanliu.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://www.cs.sfu.ca/~pingtan/" target="_blank"> <img alt src="images/team_pingtan.png" height="100"/> </a> </center></td>
              <td> <center> <a href="https://hughw19.github.io/" target="_blank"> <img alt src="images/team_hewang.png" height="100"/> </a> </center></td>
            </tr>
            <tr>
              <td>  <center>  <font size= "2">Qiyu Dai<sup>1*</sup></font> </center></td>
              <td>  <center>  <font size= "2">Yan Zhu<sup>1*</sup></font></center> </td>
              <td>  <center>  <font size= "2">Yiran Geng<sup>1</sup></font></center> </td>
              <td>  <center>  <font size= "2">Ciyu Ruan<sup>2</sup></font> </center></td>
              <td>  <center>  <font size= "2">Jiazhao Zhan<sup>1</sup></font> </center></td>
              <td> <center>   <font size= "2">He Wang<sup>1†</sup></font> </center></td>
            </tr>
            </p>
            </table>
        </div>
        <div class="col-12">
          <p> <font size= "2"><sup>1</sup> Peking University</font> &nbsp;
            <font size= "2"><sup>2</sup> National University of Defense Technology</font> &nbsp;

            <br>
            <font size= "2"><sup>*</sup> equal contributions</font> &nbsp;
            <font size= "2"><sup>†</sup> corresponding author</font> &nbsp;
          </p>
        </div>
      </div>
    </div>
  </section> -->
  <br>
            
  <!-- citing -->
  <div class="container" style="width:58%">
    <div class="row ">
      <div class="col-12">
          <h2><strong>Citation</strong></h2>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{Dai2022GraspNeRFM6,
  title={GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF},
  author={Qiyu Dai and Yan Zhu and Yiran Geng and Ciyu Ruan and Jiazhao Zhang and He Wang},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.06575}
}</code></pre>
      </div>
    </div>
  </div>
  <br>

    <!-- Contact -->
    <div class="container" style="width:58%">
      <div class="row ">
        <div class="col-12">
            <h2><strong>Contact</strong></h2>
            <hr style="margin-top:0px">
            <p>If you have any questions, please feel free to contact 
              <b>Qiyu Dai</b> at qiyudai at pku dot edu dot cn, 
              <b>Yan Zhu</b> at zhuyan_ at stu dot pku dot edu dot cn,
              and <b>He Wang</b> at hewang at pku dot edu dot cn </p>
        </pre>
        </div>
      </div>
    </div>



  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
