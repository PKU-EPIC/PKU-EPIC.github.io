<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GAPartNet</title>
    <!-- Bootstrap -->
    <link rel="preconnect" href="https://rsms.me/">
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="css/main.css" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css"> -->
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left; 
        font-family: "Inter", 'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation 
              via Generalizable and Actionable Parts</h2>
            <h4 style="color:#6e6e6e;"> CVPR 2023 <font color='red'>Highlight</font> </h4>
            <hr>
            <h6> 
              <a href="https://geng-haoran.github.io/" target="_blank">Haoran Geng</a><sup>1, 2, 3*</sup>&nbsp; &nbsp;
              <a href="https://helinxu.github.io/" target="_blank">Helin Xu</a><sup>4*</sup> &nbsp; &nbsp;
              Chengyang Zhao<sup>1*</sup>&nbsp; &nbsp;
              <a href="https://chaoxu.xyz/#bio" target="_blank">Chao Xu</a><sup>5</sup>&nbsp; &nbsp;
              <a href="https://ericyi.github.io/" target="_blank">Li Yi</a><sup>4</sup> &nbsp; &nbsp;
              <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a><sup>3</sup> &nbsp; &nbsp;
              <a href="https://hughw19.github.io/" target="_blank">He Wang</a><sup>1,2†</sup>&nbsp; &nbsp;
              <br>
              <br>
            <p> 
              <sup>1</sup>CFCS, Peking University&nbsp; &nbsp; 
              <sup>2</sup>School of EECS, Peking University&nbsp; &nbsp;  
              <sup>3</sup>Beijing Institute for General Artificial Intelligence&nbsp; &nbsp; 
              <sup>4</sup>Tsinghua University&nbsp; &nbsp; 
              <sup>5</sup>University of California, Los Angeles&nbsp; &nbsp;
              <br>
            </p>
            <p> <sup>*</sup> equal contributions &nbsp;
              <sup>†</sup> corresponding author &nbsp;
              <br>
          </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            
            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5">
                    <a class="btn btn-large btn-light" href="https://arxiv.org/abs/2211.05272" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-EPIC/GAPartNet" role="button" target="_blank">
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://forms.gle/3qzv8z5vP2BT5ARN7" role="button" target="_blank">
                <i class="fa fa-github-alt"></i> Dataset </a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="./index.html" role="button" target="_blank" style="pointer-events: none">
                <i class="fa fa-github-alt"></i> Dataset (Coming soon) </a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- teaser -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive graph" width="95%"/>
              <br>
            </div>
            <p class="text-justify">
              We propose to learn generalizable object perception and manipulation skills via <b>G</b>eneralizable and <b>A</b>ctionable <b>Parts</b>, 
                and present <b>GAPartNet</b>, a large-scale interactive dataset with rich part annotations. We propose a domain generalization 
                method for cross-category part segmentation and pose estimation. Our GAPart definition boosts cross-category object 
                manipulation and can transfer to real.
            </p>
              <!-- </div> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- video -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Video</strong></h2>
            <hr style="margin-top:0px">
            <iframe width="750" height="450" src="https://www.youtube.com/watch?v=cgVFAydWpdk" 
            title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; 
            clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            allowfullscreen></iframe>
            <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
              <video width="80%" playsinline="" preload="" muted="" controls>
                <source src="videos/video_new.mp4" type="video/mp4">
              </video>
            </div>   -->
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Abstract</strong></h2>
            <hr style="margin-top:0px">
            <p class="text-justify">
              For years, researchers have been devoted to generalizable object perception and manipulation, 
              where crosscategory generalizability is highly desired yet underexplored. In this work, 
              we propose to learn such crosscategory skills via <b>G</b>eneralizable and <b>A</b>ctionable <b>Parts</b> (<b>GAParts</b>). 
              By identifying and defining 9 GAPart classes (lids, handles, etc.) in 27 object categories, 
              we construct a large-scale part-centric interactive dataset, <b>GAPartNet</b>, where we provide rich, 
              part-level annotations (semantics, poses) for 8,489 part instances on 1,166 objects. Based on GAPartNet, 
              we investigate three <b>cross-category</b> tasks: part segmentation, part pose estimation, 
              and part-based object manipulation. Given the significant domain gaps between seen and unseen object categories, 
              we propose a robust 3D segmentation method from the perspective of domain generalization by 
              integrating adversarial learning techniques. Our method outperforms all existing methods by 
              a large margin, no matter on seen or unseen categories. Furthermore, with part segmentation 
              and pose estimation results, we leverage the GAPart pose definition to design part-based manipulation 
              heuristics that can generalize well to unseen object categories in both the simulator and the real world. 
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- dataset -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Dataset</strong></h2>
            <hr style="margin-top:0px">
            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>GAPart Definition</b></h3>      
              <p class="text-justify">
                <div class="row justify-content-center" style="align-items:center; display:flex;">
                  <img src="images/GAPart.jpg" alt="input" class="img-responsive graph" width="65%"/>
                </div>
                We give a rigorous definition to the <b>GAPart classes</b>, which not only are <b>Generalizable 
                to visual recognition</b> but also <b>share similar Actionability</b>, corresponding to the G 
                and A in GAPartNet. Our main purpose of such a definition is to bridge the perception
                 and manipulation, to allow joint learning of both vision and interaction. Accordingly, 
                 we propose two principles to follow: firstly, <b>geometric similarity within part classes</b>, 
                 and secondly, <b>actionability alignment within part classes</b>.
              </p>
              <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>GAPartNet Dataset</b></h3>      
              <p class="text-justify">
                <div class="row justify-content-center" style="align-items:center; display:flex;">
                  <img src="images/dataset.jpg" alt="input" class="img-responsive graph" width="95%"/>
                </div>
                <div class="row justify-content-center" style="align-items:center; display:flex;">
                  <img src="images/dataset_statistic.jpg" alt="input" class="img-responsive graph" width="95%"/>
                </div>
                <b>Following the GAPart definition, we construct a large-scale part-centric interactive 
                dataset, GAPartNet, with rich, part-level annotations for both perception and interaction 
                tasks.</b> Our 3D object shapes come from two existing datasets, PartNet-Mobility and AKB-48,
                which are cleaned and provided with new uniform annotations based on our GAPart definition.
                The final GAPartNet has <b>9 GAPart classes</b>, providing semantic labels and pose annotations 
                for <b>8,489 GAPart instances</b> on <b>1,166 objects</b> from <b>27 object categories</b>. On average, 
                each object has 7.3 functional parts. Each GAPart class can be seen on objects from
                more than 3 object categories, and each GAPart class is found in 8.8 object categories
                on average, which lays the foundation for our benchmark on generalizable parts.
              </p>
            <br>

        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <!-- Methods -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Methods</strong></h2>
            <hr style="margin-top:0px">
            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Full pipeline</b></h3>
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/pipeline.png" alt="input" class="img-responsive graph" width="95%"/>
              </div>
              <p class="text-justify">
                <b>An Overview of Our Domain-generalizable Part Segmentation and Pose Estimation Method.</b>  
                We introduce a part-oriented domain adversarial training strategy that can tackle multi-resolution 
                features and distribution imbalance for the domain-invariant GAPart feature extraction. 
                The training strategy tackles the challenges in our tasks and dataset, significantly improving the 
                generalizability of our method for part segmentation and pose estimation.
              </p>
            <br>


            <!-- <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Grasping Policy</b></h3>
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img style="padding: 0.5em 0 1em 0;" src="images/policyPipeline.png" alt="input" class="img-responsive graph" width="85%"/>
            </div>
            <p class="text-justify">
              <b>The goal-conditioned dexterous grasping policy pipeline</b>. 
              $\widetilde{{\mathcal{S}}^{\mathcal{E}}}=(\widetilde{\bm{s}_r},\widetilde{\bm{s}_o},X_O,\widetilde{g})$ 
              and $\widetilde{{\mathcal{S}}^{\mathcal{S}}}=(\widetilde{\bm{s}_r},\widetilde{X_S},\widetilde{g})$ denote 
              the input state of the teacher policy and student policy after state canonicalization, respectively;
              $\oplus$ denotes concatenation.
            </p>
            <br> -->
            <!-- <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Robotic Grasping</b></h3> -->
            <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
              <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
                <source src="videos/grasping.mp4" type="video/mp4">
              </video>
            </div>   -->
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

    <!-- Results -->
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2><strong>Results</strong></h2>
              <hr style="margin-top:0px">
                <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Cross-Category Part Segmentation</b></h3>  
                We visualize the results of the different methods on the seen and unseen categories,
                 where the red blocks show the failures. Our method has fewer failure cases. 
                 In particular, on the unseen category, our method still achieves the desired performance
                  when the performance of other methods drops significantly.    
                <p class="text-justify">
                  <div class="row justify-content-center" style="align-items:center; display:flex;">
                    <img src="images/segmentation.jpg" alt="input" class="img-responsive graph" width="95%"/>
                  </div>
                </p>
                <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Cross-Category Part Pose Estimation</b></h3>  
                We visualize the results of the part pose estimation. Our method has better results 
                on seen and unseen categories with better generalization across categories.
                <p class="text-justify">
                  <div class="row justify-content-center" style="align-items:center; display:flex;">
                    <img src="images/pose.jpg" alt="input" class="img-responsive graph" width="95%"/>
                  </div>
                </p>
                The following shows the visualization results for these two tasks. The left two figures 
                show the results of cross-category part segmentation and pose estimation on seen and 
                unseen categories, while the right figure shows some of the failure cases. Here we 
                only show the revolute joint estimation results.
                <p class="text-justify">
                  <div class="row justify-content-center" style="align-items:center; display:flex;">
                    <img src="images/perception.jpg" alt="input" class="img-responsive graph" width="95%"/>
                  </div>
                </p>
                <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Cross-Category Part-based Object Manipulation</b></h3>      
                The following is a visualization of our part-based object manipulatio.
                
                Experiments in the simulator show that our approach enables more human-like interactions,
                while the RL algorithm from the ManiSkill benchmark often tries to open a door or drawer 
                by prying and rubbing the edge of the door or drawer, rather than using the handle.
                <p class="text-justify">
                  <div class="row justify-content-center" style="align-items:center; display:flex;">
                    <img src="images/simulator.gif" alt="input" class="img-responsive graph" width="95%"/>
                  </div>
                </p>
                We further tested our method in real-world experiments, showing that our method is 
                robust to Domain Gap, which allows us to generate reliable part segmentation and 
                part pose estimation, and ultimately to successfully manipulate parts from unseen objects.
                <p class="text-justify">
                  <div class="row justify-content-center" style="align-items:center; display:flex;">
                    <img src="images/real1.gif" alt="input" class="img-responsive graph" width="95%"/>
                  </div>
                </p>
                <p class="text-justify">
                  <div class="row justify-content-center" style="align-items:center; display:flex;">
                    <img src="images/real2.gif" alt="input" class="img-responsive graph" width="95%"/>
                  </div>
                </p>
              <br>
  
          </div>
          </div>
        </div>
      </div>
    </section>
    <br>

<!--   
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Language-guided Dexterous Grasping</strong></h2>
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/language.png" alt="input" class="img-responsive graph" width="60%"/>
              </div>
              <p class="text-justify">
                <b>Qualitative results of language-guided grasp proposal selection</b>.
                CLIP can select proposals complying with the language instruction, 
                allowing goal-conditioned policy to execute potentially functional grasps.
              </p>
            <br>
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Qualitative results</strong></h2>
          <hr style="margin-top:0px">
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/gallery2.png" alt="input" class="img-responsive graph" width="60%"/>
          </div>
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/gallery.png" alt="input" class="img-responsive graph" width="100%"/>
          </div>
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br> -->

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h2><strong>Citation</strong></h2>
          <hr style="margin-top:0px">
              <!-- <pre style="background-color: #e9eeef;padding: 1.25em 1.5em"> -->
<pre style="background-color: #e9eeef;padding: 0 1.5em">
<code>
@article{geng2022gapartnet,
  title={GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts},
  author={Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
  journal={arXiv preprint arXiv:2211.05272},
  year={2022}
}
</code>
</pre>
      </div>
    </div>
  </div>
  <br>

    <!-- Contact -->
    <div class="container">
      <div class="row ">
        <div class="col-12">
            <h2><strong>Contact</strong></h2>
            <hr style="margin-top:0px">
            <p>If you have any questions, please feel free to contact us:
              <ul>
                <li><b>Haoran Geng</b>&colon; ghr<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>stu.pku.edu.cn </li>
                <li><b>Helin Xu</b>&colon; xuhelin1911<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>gmail.com </li>
                <li><b>Chengyang Zhao</b>&colon; zhaochengyang<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>pku.edu.cn </li>
                <li><b>He Wang</b>&colon; hewang<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>pku.edu.cn </li>
                <!-- <li><b>Weikang Wan</b>&colon; wwk<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>pku.edu.cn </li>
                <li><b>Jialiang Zhang</b>&colon; jackzhang0906<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>126.com </li>
                <li><b>Haoran Liu</b>&colon; lhrrhl0419<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>163.com </li>
                <li><b>He Wang</b>&colon; hewang<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>pku.edu.cn </li> -->
              </ul>
            </p>
        </pre>
        </div>
      </div>
    </div>

    <a href="https://hits.seeyoufarm.com">
      <img id="myImage" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fpku-epic.github.io%2FGAPartNet%2F&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/>
      <script>
        function hideImage() {
            document.getElementById("myImage").style.display = "none";
        }
        window.onload = hideImage;
        </script>
    </a>

  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']],
      macros: {
        bm: ["{\\boldsymbol #1}",1],
      }}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
